{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ComputerVisionPart3_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xnMOsbqHz61"
      },
      "source": [
        "## M2L: Image to Image Translation Tutorial (PART III)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riM2huyZjKZ-"
      },
      "source": [
        "### Image to image translation using conditional GAN's, as described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n",
        "### by Marco Buzzelli, Luigi Celona, Flavio Piccoli, and Simone Zini\n",
        "\n",
        "* Excercise: Convert building facades to real buildings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7drmZ2pj2F-"
      },
      "source": [
        "We will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep our example short, we will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
        "\n",
        "Each epoch takes around 15 seconds on a single V100 GPU.\n",
        "\n",
        "Below is the output generated after training the model for 200 epochs.\n",
        "\n",
        "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
        "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Import JAX, Haiku, and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmG8rR8D1fZa"
      },
      "source": [
        "!pip install ipdb &> /dev/null\n",
        "!pip install git+https://github.com/deepmind/dm-haiku &> /dev/null\n",
        "!pip install -U tensorboard &> /dev/null\n",
        "!pip install git+https://github.com/deepmind/optax.git &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "# Dataset libraries.\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import optax  # Package for optimizer.\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Plotting libraries.\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "from typing import Mapping, Optional, Tuple, NamedTuple, Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
        "\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn-k8kTXuAlv"
      },
      "source": [
        "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
        "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
        "                                      origin=_URL,\n",
        "                                      extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjMDvmR_j2GA"
      },
      "source": [
        "## Hyper-parameters for data preprocessing and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CbTEt448b4R"
      },
      "source": [
        "BUFFER_SIZE = 400  #@param\n",
        "BATCH_SIZE = 1  #@param\n",
        "IMG_WIDTH = 256  #@param\n",
        "IMG_HEIGHT = 256  #@param\n",
        "TRAIN_INIT_RANDOM_SEED = 1729  #@param\n",
        "LAMBDA = 100  #@param\n",
        "EPOCHS = 150\n",
        "\n",
        "# We need a random key for initialization.\n",
        "rng = jax.random.PRNGKey(TRAIN_INIT_RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9ZAGH5K3SY"
      },
      "source": [
        "#@title Dataset loading and preprocessing\n",
        "# We use tensorflow readers; JAX does not have support for input data reading\n",
        "# and pre-processing.\n",
        "def load(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "\n",
        "    w = tf.shape(image)[1]\n",
        "\n",
        "    w = w // 2\n",
        "    real_image = image[:, :w, :]\n",
        "    input_image = image[:, w:, :]\n",
        "\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OLHMpsQ5aOv"
      },
      "source": [
        "inp, re = load(PATH + 'train/100.jpg')\n",
        "# Casting to int for matplotlib to show the image.\n",
        "plt.figure()\n",
        "plt.imshow(inp/255.0)\n",
        "plt.figure()\n",
        "plt.imshow(re/255.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwwYQpu9FzDu"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "    input_image = tf.image.resize(input_image, [height, width],\n",
        "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, [height, width],\n",
        "                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3IwqhiIszt"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "    cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhR2cgbLKWW"
      },
      "source": [
        "# Normalizes the input images to [-1, 1].\n",
        "def normalize(input_image, real_image):\n",
        "    input_image = (input_image / 127.5) - 1\n",
        "    real_image = (real_image / 127.5) - 1\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2RHtcYPj2GC"
      },
      "source": [
        "Random jittering as described in the paper is composed of the following steps:\n",
        "1. Resize an image to a bigger height and width\n",
        "2. Randomly crop to the target size\n",
        "3. Randomly flip the image horizontally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVQOjcPVLrUc"
      },
      "source": [
        "#@title Data augmentation { form-width: \"40%\"}\n",
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "    # Resizing to 286 x 286 x 3.\n",
        "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "\n",
        "    # Randomly cropping to 256 x 256 x 3.\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        # Random mirroring.\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0OGdi6D92kM"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(4):\n",
        "    rj_inp, rj_re = random_jitter(inp, re)\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.imshow(rj_inp / 255.0)\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyaP4hLJ8b4W"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = random_jitter(input_image, real_image)\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3Z6D_zKSru"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = resize(input_image, real_image,\n",
        "                                     IMG_HEIGHT, IMG_WIDTH)\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGN6ouoQxt3"
      },
      "source": [
        "## Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQHmYSmk8b4b"
      },
      "source": [
        "train_dataset = tf.data.Dataset.list_files(PATH + 'train/*.jpg')\n",
        "train_dataset = train_dataset.map(load_image_train,\n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9J0yA58b4g"
      },
      "source": [
        "test_dataset = tf.data.Dataset.list_files(PATH + 'test/*.jpg')\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Build the Generator\n",
        "The architecture of the generator is a modified U-Net\n",
        "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
        "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout (applied to the first 3 blocks) -> ReLU)\n",
        "  * There are skip connections between the encoder and decoder (as in U-Net)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R09ATE_SH9P"
      },
      "source": [
        "#@title Encoder definition (Conv -> Batchnorm -> Leaky ReLU) { form-width: \"40%\" }\n",
        "class Encoder(hk.Module):\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 size: int,\n",
        "                 apply_batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.apply_batchnorm = apply_batchnorm\n",
        "\n",
        "    def __call__(self, inputs, is_training):\n",
        "        # Encoder steps:\n",
        "        # 1. conv layer (channels, size, stride=2, init, pad='SAME', nobias)\n",
        "        out = hk.Conv2D(self.channels, self.size, stride=2, padding=\"SAME\",\n",
        "                        w_init=self.initializer, with_bias=False)(inputs)\n",
        "\n",
        "        # 2. batch_norm\n",
        "        if self.apply_batchnorm:\n",
        "            bn = hk.BatchNorm(create_scale=True, create_offset=True,\n",
        "                              decay_rate=0.999, eps=0.001)\n",
        "            out = bn(out, is_training)\n",
        "\n",
        "        # 3. leakyReLU (negative_slop=0.2)\n",
        "        out = jax.nn.leaky_relu(out, negative_slope=0.2)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgDsHClSQzP"
      },
      "source": [
        "#@title Decoder definition (Transposed Conv -> Batchnorm -> Dropout (applied to the first 3 blocks) -> ReLU)  { form-width: \"40%\" }\n",
        "class Decoder(hk.Module):\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 size: int,\n",
        "                 apply_dropout=False):\n",
        "        super().__init__()\n",
        "        self.initializer = hk.initializers.RandomNormal(mean=0.0,\n",
        "                                                        stddev=0.02)\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.apply_dropout = apply_dropout\n",
        "\n",
        "    def __call__(self, inputs, is_training):\n",
        "        # Decoder steps:\n",
        "        # 1. transpose conv layer (channels, size, stride=2, init, pad='SAME', nobias)\n",
        "        out = hk.Conv2DTranspose(self.channels, self.size, stride=2,\n",
        "                                 padding='SAME', w_init=self.initializer,\n",
        "                                 with_bias=False)(inputs)\n",
        "        \n",
        "        # 2. batch_norm\n",
        "        out = hk.BatchNorm(create_scale=True, create_offset=True,\n",
        "                           decay_rate=0.999, eps=0.001)(out, is_training)\n",
        "\n",
        "        # 3. dropout\n",
        "        if self.apply_dropout and is_training:\n",
        "            # Apply 0.5 probability dropout to out\n",
        "            out = hk.dropout(rng, rate=0.5, x=out)\n",
        "\n",
        "        # 4. ReLU\n",
        "        out = jax.nn.relu(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFPI4Nu-8b4q"
      },
      "source": [
        "class Generator(hk.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # In comment the output size of each block. `bs` is the batch size.\n",
        "        self.down_stack = [\n",
        "            Encoder(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
        "            Encoder(128, 4),  # (bs, 64, 64, 128)\n",
        "            Encoder(256, 4),  # (bs, 32, 32, 256)\n",
        "            Encoder(512, 4),  # (bs, 16, 16, 512)\n",
        "            Encoder(512, 4),  # (bs, 8, 8, 512)\n",
        "            Encoder(512, 4),  # (bs, 4, 4, 512)\n",
        "            Encoder(512, 4),  # (bs, 2, 2, 512)\n",
        "            Encoder(512, 4),  # (bs, 1, 1, 512)\n",
        "        ]\n",
        "\n",
        "        self.up_stack = [\n",
        "            Decoder(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
        "            Decoder(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
        "            Decoder(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
        "            Decoder(512, 4),  # (bs, 16, 16, 1024)\n",
        "            Decoder(256, 4),  # (bs, 32, 32, 512)\n",
        "            Decoder(128, 4),  # (bs, 64, 64, 256)\n",
        "            Decoder(64, 4),  # (bs, 128, 128, 128)\n",
        "        ]\n",
        "\n",
        "        initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.last = hk.Conv2DTranspose(3, 4,\n",
        "                                       stride=2,\n",
        "                                       padding='SAME',\n",
        "                                       w_init=initializer)  # (bs, 256, 256, 3)\n",
        "\n",
        "    def __call__(self, x, is_training):\n",
        "        # Downsampling through the model\n",
        "        skips = []\n",
        "        for down in self.down_stack:\n",
        "            x = down(x, is_training)\n",
        "            skips.append(x)\n",
        "\n",
        "        # Upsampling and establishing the skip connections\n",
        "        skips = reversed(skips[:-1])\n",
        "        for up, skip in zip(self.up_stack, skips):\n",
        "            x = up(x, is_training)\n",
        "            x = jnp.concatenate([x, skip], axis=-1)\n",
        "\n",
        "        x = self.last(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpDPEQXIAiQO"
      },
      "source": [
        "### Generator loss\n",
        "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
        "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
        "  * This allows the generated image to become structurally similar to the target image.\n",
        "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuzZAKFjqzqA"
      },
      "source": [
        "# Computes binary cross entropy for classification.\n",
        "def bce_w_logits(\n",
        "    logits: jnp.ndarray,\n",
        "    target: jnp.ndarray\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"\n",
        "    Binary Cross Entropy Loss\n",
        "    :param logits: Input tensor\n",
        "    :param target: Target tensor\n",
        "    :return: Scalar value\n",
        "    \"\"\"\n",
        "    max_val = jnp.clip(logits, 0, None)\n",
        "    loss = logits - logits * target + max_val + \\\n",
        "    jnp.log(jnp.exp(-max_val) + jnp.exp((-logits - max_val)))\n",
        "\n",
        "    return jnp.mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "source": [
        "def generator_loss(\n",
        "    disc_generated_output: jnp.ndarray,\n",
        "    gen_output: jnp.ndarray,\n",
        "    target: jnp.ndarray\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Computes the generator loss for the given batch.\"\"\"\n",
        "    gan_loss = bce_w_logits(disc_generated_output,\n",
        "                            jnp.ones_like(disc_generated_output))\n",
        "\n",
        "    # Mean absolute error.\n",
        "    l1_loss = jnp.mean(jnp.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlB-XMY5Awj9"
      },
      "source": [
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKZfoaoEF22"
      },
      "source": [
        "## Build the Discriminator\n",
        "The Discriminator is a PatchGAN\n",
        "\n",
        "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
        "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
        "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
        "  * Discriminator receives 2 inputs.\n",
        "    * Input image and the target image, which it should classify as real.\n",
        "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
        "    * We concatenate these 2 inputs together in the code (`jax.numpy.concatenate([inp, tar], axis=-1)`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll6aNeQx8b4v"
      },
      "source": [
        "class Discriminator(hk.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "        self.down1 = Encoder(64, 4, apply_batchnorm=False)\n",
        "        self.down2 = Encoder(128, 4)\n",
        "        self.down3 = Encoder(256, 4)\n",
        "\n",
        "        self.conv = hk.Conv2D(512, 4, stride=1, w_init=initializer,\n",
        "                              padding='VALID', with_bias=False)\n",
        "        self.bn = hk.BatchNorm(create_scale=True, create_offset=True,\n",
        "                               decay_rate=0.999, eps=0.001)\n",
        "        self.last = hk.Conv2D(1, 4, stride=1, padding='VALID',\n",
        "                              w_init=initializer)\n",
        "\n",
        "    def __call__(self, x, is_training):  # (bs, 256, 256, channels*2)\n",
        "        x = self.down1(x, is_training)  # (bs, 128, 128, 64)\n",
        "        x = self.down2(x, is_training)  # (bs, 64, 64, 128)\n",
        "        x = self.down3(x, is_training)  # (bs, 32, 32, 256)\n",
        "        x = jnp.pad(x, ((0, 0), (1, 1), (1, 1), (0, 0)))  # (bs, 34, 34, 256)\n",
        "        x = self.conv(x)  # (bs, 31, 31, 512)\n",
        "        x = self.bn(x, is_training)\n",
        "        x = jax.nn.leaky_relu(x, negative_slope=0.2)\n",
        "        x = jnp.pad(x, ((0, 0), (1, 1), (1, 1), (0, 0)))  # (bs, 33, 33, 256)\n",
        "        x = self.last(x)  # (bs, 30, 30, 1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqg1dhUAWoD"
      },
      "source": [
        "### Discriminator loss\n",
        "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
        "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones (since these are the real images)**\n",
        "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**\n",
        "  * Then the total_loss is the sum of real_loss and the generated_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = bce_w_logits(disc_real_output,\n",
        "                             jnp.ones_like(disc_real_output))\n",
        "    generated_loss = bce_w_logits(disc_generated_output,\n",
        "                                  jnp.zeros_like(disc_generated_output))\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ede4p2YELFa"
      },
      "source": [
        "The training procedure for the discriminator is shown below.\n",
        "\n",
        "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS9sHa-1BoAF"
      },
      "source": [
        "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLKOG55MErD0"
      },
      "source": [
        "## Training\n",
        "\n",
        "* For each example input generates an output.\n",
        "* The discriminator receives the input image and the generated image as the first input. The second input is the input image and the target image.\n",
        "* Next, we calculate the generator and the discriminator loss.\n",
        "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\n",
        "* Last, we log the losses to TensorBoard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "### Define the Checkpoint-saver\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7micePl8XVtF"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "if not os.path.exists(checkpoint_prefix):\n",
        "    os.makedirs(checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh0Q_XuscND5"
      },
      "source": [
        "class P2PTuple(NamedTuple):\n",
        "    gen: Any\n",
        "    disc: Any\n",
        "\n",
        "\n",
        "class P2PState(NamedTuple):\n",
        "    params: P2PTuple\n",
        "    states: P2PTuple\n",
        "    opt_state: P2PTuple\n",
        "\n",
        "\n",
        "class Pix2Pix:\n",
        "    \"\"\"Pix2Pix model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gen_transform = hk.transform_with_state(\n",
        "            lambda *args: Generator()(*args)\n",
        "        )\n",
        "        self.disc_transform = hk.transform_with_state(\n",
        "            lambda *args: Discriminator()(*args)\n",
        "        )\n",
        "\n",
        "        # Build the optimizers.\n",
        "        self.gen_optimizer = optax.adam(2e-4, b1=0.5, b2=0.999)\n",
        "        self.disc_optimizer = optax.adam(2e-4, b1=0.5, b2=0.999)\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnums=0)\n",
        "    def initial_state(self,\n",
        "                      rng: jnp.ndarray,\n",
        "                      batch: Tuple[jnp.ndarray, jnp.ndarray]):\n",
        "        \"\"\"Returns the initial parameters and optimize states of the generator.\n",
        "        \"\"\"\n",
        "        rng, gen_rng, disc_rng = jax.random.split(rng, 3)\n",
        "        gen_params, gen_state = self.gen_transform.init(gen_rng, batch[0], True)\n",
        "        disc_params, disc_state = \\\n",
        "            self.disc_transform.init(disc_rng,\n",
        "                                     jnp.concatenate(batch, axis=-1),\n",
        "                                     True)\n",
        "        params = P2PTuple(gen=gen_params, disc=disc_params)\n",
        "        states = P2PTuple(gen=gen_state, disc=disc_state)\n",
        "\n",
        "        # Initialize the optimizers.\n",
        "        opt_state = P2PTuple(gen=self.gen_optimizer.init(params.gen),\n",
        "                             disc=self.disc_optimizer.init(params.disc)\n",
        "                             )\n",
        "        return P2PState(params=params, states=states, opt_state=opt_state)\n",
        "\n",
        "    def generate_images(self,\n",
        "                        params: P2PTuple,\n",
        "                        state: P2PTuple,\n",
        "                        test_input):\n",
        "        # Note: The `training=True` is intentional here since\n",
        "        #       we want the batch statistics while running the model\n",
        "        #       on the test dataset. If we use training=False, we will get\n",
        "        #       the accumulated statistics learned from the training dataset\n",
        "        #       (which we don't want)\n",
        "        prediction, _ = self.gen_transform.apply(\n",
        "            params, state, None, test_input, True\n",
        "        )\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def gen_loss(self,\n",
        "                 gen_params: P2PTuple,\n",
        "                 gen_state: P2PTuple,\n",
        "                 batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "                 disc_params: P2PTuple,\n",
        "                 disc_state: P2PTuple,\n",
        "                 rng_gen, rng_disc):\n",
        "        \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
        "\n",
        "        input, target = batch\n",
        "        \n",
        "        output, gen_state = self.gen_transform.apply(\n",
        "            gen_params, gen_state, rng_gen, input, True\n",
        "        )\n",
        "\n",
        "        # Evaluate using the discriminator.\n",
        "        disc_generated_output, disc_state = self.disc_transform.apply(\n",
        "            disc_params, disc_state, rng_disc,\n",
        "            jnp.concatenate([input, output], axis=-1), True\n",
        "            )\n",
        "\n",
        "        states = P2PTuple(gen=gen_state, disc=disc_state)\n",
        "\n",
        "        # Compute discriminator loss.\n",
        "        total_loss, gan_loss, l1_loss = generator_loss(\n",
        "            disc_generated_output, output, target\n",
        "            )\n",
        "\n",
        "        return total_loss, (output, states, gan_loss, l1_loss)\n",
        "\n",
        "    def disc_loss(self,\n",
        "                  params: P2PTuple,\n",
        "                  state: P2PTuple,\n",
        "                  batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "                  gen_output: jnp.ndarray, rng):\n",
        "        \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
        "        input, target = batch\n",
        "        real_output, state = self.disc_transform.apply(\n",
        "            params, state, rng, jnp.concatenate([input, target], axis=-1), True\n",
        "        )\n",
        "\n",
        "        generated_output, state = self.disc_transform.apply(\n",
        "            params, state, rng,\n",
        "            jnp.concatenate([input, gen_output], axis=-1), True\n",
        "        )\n",
        "\n",
        "        # Compute discriminator loss.\n",
        "        loss = discriminator_loss(real_output, generated_output)\n",
        "        return loss, state\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnums=0)\n",
        "    def update(self, rng, p2p_state, batch):\n",
        "        \"\"\" Performs a parameter update. \"\"\"\n",
        "        rng, gen_rng, disc_rng = jax.random.split(rng, 3)\n",
        "\n",
        "        # Update the generator.\n",
        "        (gen_loss, gen_aux), gen_grads = \\\n",
        "            jax.value_and_grad(self.gen_loss,\n",
        "                               has_aux=True)(\n",
        "            p2p_state.params.gen,\n",
        "            p2p_state.states.gen,\n",
        "            batch,\n",
        "            p2p_state.params.disc,\n",
        "            p2p_state.states.disc,\n",
        "            gen_rng, disc_rng)\n",
        "\n",
        "        generated_output, states, gan_loss, l1_loss = gen_aux\n",
        "        gen_update, gen_opt_state = self.gen_optimizer.update(\n",
        "            gen_grads, p2p_state.opt_state.gen)\n",
        "        gen_params = optax.apply_updates(p2p_state.params.gen, gen_update)\n",
        "\n",
        "        # Update the discriminator.\n",
        "        (disc_loss, disc_state), disc_grads = \\\n",
        "            jax.value_and_grad(self.disc_loss,\n",
        "                               has_aux=True)(\n",
        "            p2p_state.params.disc,\n",
        "            states.disc,\n",
        "            batch,\n",
        "            generated_output,\n",
        "            disc_rng)\n",
        "\n",
        "        disc_update, disc_opt_state = self.disc_optimizer.update(\n",
        "            disc_grads, p2p_state.opt_state.disc)\n",
        "        disc_params = optax.apply_updates(p2p_state.params.disc, disc_update)\n",
        "\n",
        "        params = P2PTuple(gen=gen_params, disc=disc_params)\n",
        "        states = P2PTuple(gen=states.gen, disc=disc_state)\n",
        "        opt_state = P2PTuple(gen=gen_opt_state, disc=disc_opt_state)\n",
        "        p2p_state = P2PState(params=params, states=states, opt_state=opt_state)\n",
        "\n",
        "        return p2p_state, gen_loss, disc_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVSnyvwjCUb"
      },
      "source": [
        "# The model.\n",
        "net = Pix2Pix()\n",
        "\n",
        "# Initialize the network and optimizer.\n",
        "for input, target in train_dataset.take(1):\n",
        "    net_state = net.initial_state(rng, (jnp.asarray(input),\n",
        "                                        jnp.asarray(target)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNNMDBNH12q-"
      },
      "source": [
        "import datetime\n",
        "log_dir = \"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7s-vBHFKdh"
      },
      "source": [
        "The actual training loop:\n",
        "\n",
        "* Iterates over the number of epochs.\n",
        "* On each epoch it clears the display, and runs `generate_images` to show it's progress.\n",
        "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
        "* It saves a checkpoint every 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "source": [
        "def fit(train_ds, epochs, test_ds, net_state):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        for example_input, example_target in test_ds.take(1):\n",
        "            prediction = net.generate_images(net_state.params.gen,\n",
        "                                             net_state.states.gen,\n",
        "                                             jnp.asarray(example_input))\n",
        "            plt.figure(figsize=(15, 15))\n",
        "\n",
        "            display_list = [example_input[0], example_target[0], prediction[0]]\n",
        "            title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "            for i in range(3):\n",
        "                plt.subplot(1, 3, i+1)\n",
        "                plt.title(title[i])\n",
        "                # Getting the pixel values between [0, 1] to plot it.\n",
        "                plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "        print(\"Epoch: \", epoch)\n",
        "\n",
        "        # Train loop.\n",
        "        for n, (input_image, target) in train_ds.enumerate():\n",
        "            # Take a training step.\n",
        "            print('.', end='')\n",
        "            if (n+1) % 100 == 0:\n",
        "                print()\n",
        "\n",
        "            net_state, gen_total_loss, disc_loss, \\\n",
        "            gen_gan_loss, gen_l1_loss = \\\n",
        "                net.update(rng, net_state,\n",
        "                           (jnp.asarray(input_image), jnp.asarray(target)))\n",
        "\n",
        "            with summary_writer.as_default():\n",
        "                tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "                tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "                tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "                tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
        "        \n",
        "        print()\n",
        "\n",
        "        # Save (checkpoint) the model every 20 epochs.\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            with open(\n",
        "                os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
        "                    'wb') as handle:\n",
        "                pickle.dump(net_state.params, handle,\n",
        "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            with open(\n",
        "                os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
        "                    'wb') as handle:\n",
        "                pickle.dump(net_state.states, handle,\n",
        "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                           time.time()-start))\n",
        "\n",
        "    # Save the last checkpoint.\n",
        "    with open(\n",
        "        os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
        "            'wb') as handle:\n",
        "        pickle.dump(net_state.params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with open(\n",
        "        os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
        "            'wb') as handle:\n",
        "        pickle.dump(net_state.states, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wozqyTh2wmCu"
      },
      "source": [
        "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
        "\n",
        "To launch the viewer run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot22ujrlLhOd"
      },
      "source": [
        "#docs_infra: no_execute\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0-8Bzg22ox"
      },
      "source": [
        "Now run the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zZmKmvOH85"
      },
      "source": [
        "fit(train_dataset, EPOCHS, test_dataset, net_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeq9sByu86-B"
      },
      "source": [
        "If you want to share the TensorBoard results _publicly_ you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
        "\n",
        "Note: This requires a Google account.\n",
        "\n",
        "```\n",
        "!tensorboard dev upload --logdir  {log_dir}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-kT7WHRKz-E"
      },
      "source": [
        "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGhS_LfwQoL"
      },
      "source": [
        "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
        "\n",
        "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
        "\n",
        "It can also included inline using an `<iframe>`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IS4c93guQ8E"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTm4peo3cem"
      },
      "source": [
        "Interpreting the logs from a GAN is more subtle than a simple classification or regression model. Things to look for:\n",
        "\n",
        "* Check that neither model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
        "* The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2: That the discriminator is on average equally uncertain about the two options.\n",
        "* For the `disc_loss` a value below `0.69` means the discriminator is doing better than random, on the combined set of real + generated images.\n",
        "* For the `gen_gan_loss` a value below `0.69` means the generator is doing better than random at fooling the descriminator.\n",
        "* As training progresses the `gen_l1_loss` should go down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz80bY3aQ1VZ"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSSm4kfvJiqv"
      },
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t4x69adQ5xb"
      },
      "source": [
        "# Restore the latest checkpoint in checkpoint_dir.\n",
        "with open(\n",
        "    os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
        "        'rb') as handle:\n",
        "    params = pickle.load(handle)\n",
        "\n",
        "with open(\n",
        "    os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
        "        'rb') as handle:\n",
        "    states = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "## Generate using test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzDgOHSHzSFG"
      },
      "source": [
        "* We pass images from the test dataset to the generator.\n",
        "* The generator will then translate the input image into the output.\n",
        "* Last step is to plot the predictions and **voila!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgSnmy2nqSP"
      },
      "source": [
        "# Run the trained model on a few examples from the test dataset\n",
        "for test_input, tar in test_dataset.take(5):\n",
        "  prediction = net.generate_images(params.gen, states.gen,\n",
        "                                   jnp.asarray(test_input))\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}